{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge prediction problem\n",
    "- To build a model for predicting category of edge\n",
    "- Reference : https://github.com/dragen1860/TensorFlow-2.x-Tutorials/tree/master/20-GCN\n",
    "- Tensorflow 2.0 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None\n",
    "pd.options.display.max_colwidth=99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = '/data/private/talkhello_dev_12307/work/homer/tf_20_tutorial/20-GCN/'\n",
    "names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "objects = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading cora data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "objects = []\n",
    "\n",
    "path = os.path.join(repo, \"data/ind.{}.{}\")\n",
    "for i in range(len(names)):\n",
    "    with open(path.format('cora', names[i]), 'rb') as f:\n",
    "        if sys.version_info > (3, 0):\n",
    "            objects.append(pkl.load(f, encoding='latin1'))\n",
    "        else:\n",
    "            objects.append(pkl.load(f))\n",
    "            \n",
    "x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "\n",
    "test_idx_path = os.path.join(repo, \"data/ind.{}.test.index\")\n",
    "test_idx_reorder = parse_index_file(test_idx_path.format('cora'))\n",
    "test_idx_range = np.sort(test_idx_reorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1708, 1433)\n",
      "Label을 가진 데이터의 갯수 : 1708\n"
     ]
    }
   ],
   "source": [
    "# allx : Feature matrix of train set (sparse)\n",
    "print(allx.shape)\n",
    "\n",
    "# ally : Lables\n",
    "print('Label을 가진 데이터의 갯수 :', sum(ally.sum(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습셋 + 테스트셋 : (2708, 1433) (2708, 7)\n"
     ]
    }
   ],
   "source": [
    "# Features\n",
    "features = sp.vstack((allx, tx)).tolil()\n",
    "labels = np.vstack((ally, ty))\n",
    "\n",
    "# 테스트 셋만 셔플을 한다.\n",
    "features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "print('학습셋 + 테스트셋 :', features.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adj shape : (2708, 2708)\n"
     ]
    }
   ],
   "source": [
    "# Adj : graph는 matrix가 아닌 dictionary의 형태이다.\n",
    "adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "print('Adj shape :', adj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse labels -> semi supervised learning setting으로 간다.\n",
    "- Num of train set : 140\n",
    "- Num of validation set : 500\n",
    "- Num of test set : 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_test = test_idx_range.tolist()  # 1708 ~\n",
    "idx_train = range(len(y))  # 0 ~ 139\n",
    "idx_val = range(len(y), len(y)+500)  # 140 ~ 639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mask(idx, l):\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = sample_mask(idx_train, labels.shape[0])  # 0 ~ 139\n",
    "val_mask = sample_mask(idx_val, labels.shape[0])  # 140 ~ 639\n",
    "test_mask = sample_mask(idx_test, labels.shape[0])  # 1708 ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.zeros(labels.shape)\n",
    "y_val = np.zeros(labels.shape)\n",
    "y_test = np.zeros(labels.shape)\n",
    "\n",
    "y_train[train_mask, :] = labels[train_mask, :]\n",
    "y_val[val_mask, :] = labels[val_mask, :]\n",
    "y_test[test_mask, :] = labels[test_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj: (2708, 2708)\n",
      "features: (2708, 1433)\n",
      "y: (2708, 7) (2708, 7) (2708, 7)\n",
      "mask: (2708,) (2708,) (2708,)\n"
     ]
    }
   ],
   "source": [
    "print('adj:', adj.shape)\n",
    "print('features:', features.shape)\n",
    "print('y:', y_train.shape, y_val.shape, y_test.shape)\n",
    "print('mask:', train_mask.shape, val_mask.shape, test_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row normalization of feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    def to_tuple(mx):\n",
    "        # COOrdinate format\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $C_{ii} = \\sum_{j}X_{ij}$\n",
    "- $C^{-1}X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowsum = np.array(features.sum(1))\n",
    "r_inv = np.power(rowsum, -1).flatten()\n",
    "r_inv[np.isinf(r_inv)] = 0.\n",
    "r_mat_inv = sp.diags(r_inv)\n",
    "features = r_mat_inv.dot(features)\n",
    "features = sparse_to_tuple(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features coordinates:: (49216, 2)\n",
      "features data:: (49216,)\n",
      "features shape:: (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "print('features coordinates::', features[0].shape)\n",
    "print('features data::', features[1].shape)\n",
    "print('features shape::', features[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency matrix\n",
    "\n",
    "$$\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}$$\n",
    "- $\\tilde{A}=A+I_N$ : the adjacency matrix of the undirected graph  $\\mathcal{G}$ with added self-connections.\n",
    "- $\\tilde{D}_{ii} = \\sum_{j}\\tilde{A}_{ij}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1)) # D\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten() # D^-0.5\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt) # D^-0.5\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo() # D^-0.5AD^0.5\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = [preprocess_adj(adj)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random.uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse.retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(x, y, sparse=False):\n",
    "    if sparse:\n",
    "        res = tf.sparse.sparse_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax_cross_entropy(preds, labels, mask):\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    # True : scale up\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(preds, labels, mask):\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    # True : scale up\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    return tf.reduce_mean(accuracy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero, dropout=0., sparse_inputs=False,\n",
    "                 activation=tf.nn.relu, bias=False, featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.num_features_nonzero = num_features_nonzero\n",
    "\n",
    "        self.weights_ = []\n",
    "        for i in range(1):\n",
    "            w = self.add_variable('weights' + str(i), shape=[input_dim, output_dim])\n",
    "            self.weights_.append(w)\n",
    "        if self.bias:\n",
    "            self.bias = self.add_variable('bias', shape=[output_dim])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x, support = inputs\n",
    "\n",
    "        if training is not False and self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        elif training is not False:\n",
    "            x = tf.nn.dropout(x, self.dropout)\n",
    "\n",
    "        supports = list()\n",
    "        \n",
    "        # range(len(support))\n",
    "        for i in range(1):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.weights_[i], sparse=self.sparse_inputs)\n",
    "            else:\n",
    "                pre_sup = self.weights_[i]\n",
    "            support = dot(support[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.bias\n",
    "\n",
    "        return self.activation(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(keras.Model):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_features_nonzero, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_features_nonzero = num_features_nonzero\n",
    "        self.layers_ = []\n",
    "\n",
    "        print('input dim:', input_dim)\n",
    "        print('output dim:', output_dim)\n",
    "        print('num_features_nonzero:', num_features_nonzero)\n",
    "        \n",
    "        self._build()\n",
    "\n",
    "        for p in self.trainable_variables:\n",
    "            print(p.name, p.shape)\n",
    "            \n",
    "    def _build(self):\n",
    "        self.layers_.append(GraphConvolution(input_dim=self.input_dim,  # 1433\n",
    "                                            output_dim=16,  # 16\n",
    "                                            num_features_nonzero=self.num_features_nonzero,\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            dropout=0.5,\n",
    "                                            sparse_inputs=True))\n",
    "\n",
    "        self.layers_.append(GraphConvolution(input_dim=16,  # 16\n",
    "                                            output_dim=self.output_dim,  # 7\n",
    "                                            num_features_nonzero=self.num_features_nonzero,\n",
    "                                            activation=lambda x: x,\n",
    "                                            dropout=0.5))\n",
    "\n",
    "        \n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x, label, mask, support = inputs\n",
    "\n",
    "        outputs = [x]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden = layer((outputs[-1], support), training)\n",
    "            outputs.append(hidden)\n",
    "        output = outputs[-1]\n",
    "\n",
    "        # Weight decay loss\n",
    "        loss = tf.zeros([])\n",
    "        for var in self.layers_[0].trainable_variables:\n",
    "            loss += 5e-4 * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        loss += masked_softmax_cross_entropy(output, label, mask)\n",
    "        acc = masked_accuracy(output, label, mask)\n",
    "\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[   0,    0],\n",
       "         [ 633,    0],\n",
       "         [1862,    0],\n",
       "         ...,\n",
       "         [1473, 2707],\n",
       "         [2706, 2707],\n",
       "         [2707, 2707]], dtype=int32),\n",
       "  array([0.25     , 0.25     , 0.2236068, ..., 0.2      , 0.2      ,\n",
       "         0.2      ]),\n",
       "  (2708, 2708))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjacency matrix\n",
    "support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0, 1274],\n",
       "        [   0, 1247],\n",
       "        [   0, 1194],\n",
       "        ...,\n",
       "        [2707,  329],\n",
       "        [2707,  186],\n",
       "        [2707,   19]], dtype=int32),\n",
       " array([0.11111111, 0.11111111, 0.11111111, ..., 0.07692308, 0.07692308,\n",
       "        0.07692308], dtype=float32),\n",
       " (2708, 1433))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Features\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim: 1433\n",
      "output dim: 7\n",
      "num_features_nonzero: (49216,)\n",
      "WARNING:tensorflow:From <ipython-input-25-09c8c6ee536b>:15: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "weights0:0 (1433, 16)\n",
      "weights0:0 (16, 7)\n"
     ]
    }
   ],
   "source": [
    "model = GCN(input_dim=features[2][1], output_dim=y_train.shape[1], num_features_nonzero=features[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = tf.convert_to_tensor(y_train)\n",
    "train_mask = tf.convert_to_tensor(train_mask)\n",
    "val_label = tf.convert_to_tensor(y_val)\n",
    "val_mask = tf.convert_to_tensor(val_mask)\n",
    "test_label = tf.convert_to_tensor(y_test)\n",
    "test_mask = tf.convert_to_tensor(test_mask)\n",
    "features = tf.SparseTensor(*features)\n",
    "support = [tf.cast(tf.SparseTensor(*support[0]), dtype=tf.float32)]\n",
    "num_features_nonzero = features.values.shape\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gcn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "0 1.9549707174301147 0.11428571492433548 \tval, d: 0.1759999841451645\n",
      "20 1.7731964588165283 0.5999999642372131 \tval, d: 0.3739999830722809\n",
      "40 1.5097137689590454 0.7285714149475098 \tval, d: 0.5600000023841858\n",
      "60 1.2669298648834229 0.8285714387893677 \tval, d: 0.6840000152587891\n",
      "80 1.0696214437484741 0.8928571343421936 \tval, d: 0.7559998631477356\n",
      "100 0.9236013889312744 0.9285714030265808 \tval, d: 0.7739998698234558\n",
      "120 0.8594609498977661 0.9285714626312256 \tval, d: 0.7899999618530273\n",
      "140 0.7561487555503845 0.9428571462631226 \tval, d: 0.7820000052452087\n",
      "160 0.6878491640090942 0.9500000476837158 \tval, d: 0.7919999361038208\n",
      "180 0.6463509202003479 0.9714285135269165 \tval, d: 0.7899999618530273\n",
      "\n",
      "test: 1.0399932861328125 0.8079999685287476\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as t:\n",
    "        loss, acc = model((features, train_label, train_mask, support))\n",
    "    grads = t.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    _, val_acc = model((features, val_label, val_mask, support), training=False)\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(epoch, float(loss), float(acc), '\\tval, d:', float(val_acc))\n",
    "        \n",
    "test_loss, test_acc = model((features, test_label, test_mask, support), training=False)\n",
    "\n",
    "print('\\ntest:', float(test_loss), float(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
